{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wHbSMACDa9Gx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install qiskit qiskit-aer qiskit-ibm-runtime qiskit_experiments pylatexenc quantuminspire snntorch qiskit tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wHbSMACDa9Gx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wHbSMACDa9Gx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Imports\n",
    "# from qiskit import QuantumCircuit, Aersimu, execute\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from src.Quantum_circuits import *\n",
    "from src.QSVAE_model import *\n",
    "from src.POVM_dataset import *\n",
    "# from torchvision import transforms, utils\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "if torch.cuda.is_available():\n",
    " dev = \"cuda:0\"\n",
    "#  dev = \"cpu\"\n",
    "else:\n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)\n",
    "torch.__version__\n",
    "\n",
    "# J_ROOT = os.readlink('/proc/%s/cwd' % os.environ['JPY_PARENT_PID'])\n",
    "# print(J_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:39:03.798065Z",
     "iopub.status.busy": "2024-10-02T10:39:03.797596Z",
     "iopub.status.idle": "2024-10-02T10:39:03.845735Z",
     "shell.execute_reply": "2024-10-02T10:39:03.844989Z",
     "shell.execute_reply.started": "2024-10-02T10:39:03.798023Z"
    },
    "id": "hmg6BrDBZZaN"
   },
   "outputs": [],
   "source": [
    "# @title Main (Global definitions)\n",
    "n = 3 # Amount of qubits\n",
    "shots = 10e1 # amount of shots taken by the quantum simulator\n",
    "first_run = True\n",
    "# Support for \"Starmon-5\" and \"AerSimulator\"\n",
    "backend_type = \"AerSimulator\"\n",
    "backend = select_backend(backend_type)\n",
    "train = True\n",
    "test = True\n",
    "val = True\n",
    "\n",
    "# Define hyperparameters\n",
    "beta = 0.819\n",
    "num_steps = 200\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "batch_train, batch_test, batch_val = (10000, 200, 1000)\n",
    "num_workers = 0\n",
    "shuffle = False\n",
    "split = [0.6, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:39:03.847066Z",
     "iopub.status.busy": "2024-10-02T10:39:03.846772Z",
     "iopub.status.idle": "2024-10-02T10:39:45.592272Z",
     "shell.execute_reply": "2024-10-02T10:39:45.591248Z",
     "shell.execute_reply.started": "2024-10-02T10:39:03.847035Z"
    },
    "id": "7ZeRrm-5THW2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and circuit saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dca1e9437248efb4939b3109e96ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 272.9964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282502336db54569ad4cb26b70d10394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16])\n",
      "The fidelity for 3 qubits is 0.9629821457879684 with 1000 samples.\n"
     ]
    }
   ],
   "source": [
    "quantum_exp = QuantumExperiment(backend, n, shots)\n",
    "quantum_exp.run_experiment()\n",
    "# quantum_exp.print_circuits_with_counts()\n",
    "train_loader, test_loader, val_loader = load_data(quantum_exp,\n",
    "                                                  first_run,\n",
    "                                                  backend,\n",
    "                                                  n,\n",
    "                                                  shots,\n",
    "                                                  split,\n",
    "                                                  [batch_train, batch_test, batch_val],\n",
    "                                                  shuffle, num_workers)\n",
    "\n",
    "# Instantiate the model for the given parameters\n",
    "model = SQVAE(n=n, batch_size=[batch_train, batch_train, batch_val],\n",
    "              beta=beta, num_steps=num_steps, learning_rate=learning_rate, device=device)\n",
    "\n",
    "# Run the model and get the fidelity for the current parameter setting\n",
    "fidelity_score = model.run(train=True, test=False, val=True,\n",
    "                           train_loader=train_loader, test_loader=test_loader,\n",
    "                           val_loader=val_loader, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T10:42:52.161492Z",
     "iopub.status.busy": "2024-10-02T10:42:52.160739Z",
     "iopub.status.idle": "2024-10-02T11:12:02.326321Z",
     "shell.execute_reply": "2024-10-02T11:12:02.314799Z",
     "shell.execute_reply.started": "2024-10-02T10:42:52.161451Z"
    },
    "id": "YSGXwHdlj83L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and circuit saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ce162920104e4f8564a6f7f681b3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 407.9185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14410a4733a048efbb6c59a5d98af4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16])\n",
      "The fidelity for 3 qubits is 0.8875487981083946 with 20000 samples.\n",
      "Dataset and circuit saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48acfc6506a44b5bbdf9c7cdc1c36b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 233.7799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0ffd6220804f8587b77c421585df9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32])\n",
      "The fidelity for 4 qubits is 0.9357445020820031 with 100000 samples.\n",
      "Dataset and circuit saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eb76c7e7ab4db29130ea7bf4d262b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 240.2352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a753db4221544de48cb2b960457aece0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64])\n",
      "The fidelity for 5 qubits is 0.8843845700412672 with 512000 samples.\n",
      "Dataset and circuit saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3100481477124a43821bdb75b835bad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 190.9593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5cee205bfb4ddd91aab8c3f614f234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128])\n",
      "The fidelity for 6 qubits is 0.9525288475065772 with 2048000 samples.\n",
      "Dataset and circuit saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c0e564e5ab4cab8a08c26d8470c8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 212.1580\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97927639b8a41a98ddcf2d31119ba42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 256])\n",
      "The fidelity for 7 qubits is 0.9385274155561478 with 8192000 samples.\n",
      "Dataset and circuit saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e47fc5119474fcb8e74420a6fa0bc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 220.8575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7abcd376a704658bb6fe50804882946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 512])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 3.17 GiB is allocated by PyTorch, and 982.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m SQVAE(n\u001b[38;5;241m=\u001b[39mn, batch_size\u001b[38;5;241m=\u001b[39m[batch_train, batch_train, batch_val],\n\u001b[0;32m     29\u001b[0m             beta\u001b[38;5;241m=\u001b[39mbeta, num_steps\u001b[38;5;241m=\u001b[39mnum_steps, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Run the model and get the fidelity for the current parameter setting\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m fidelity_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Append the fidelity score to the list\u001b[39;00m\n\u001b[0;32m     36\u001b[0m fidelities\u001b[38;5;241m.\u001b[39mappend(fidelity_score)\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\OneDrive\\Studie\\Embedded_Systems\\Thesis\\Code\\Python\\src\\QSVAE_model.py:241\u001b[0m, in \u001b[0;36mSQVAE.run\u001b[1;34m(self, train, test, val, train_loader, test_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage test loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val:\n\u001b[1;32m--> 241\u001b[0m     prob_rec, prob_true \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     rho_rec, rho_true \u001b[38;5;241m=\u001b[39m reconstruct_matrix_from_prob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn,s_vectors, prob_rec, prob_true)\n\u001b[0;32m    243\u001b[0m     fidelity_score \u001b[38;5;241m=\u001b[39m fidelity(rho_rec, rho_true)\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\OneDrive\\Studie\\Embedded_Systems\\Thesis\\Code\\Python\\src\\QSVAE_model.py:386\u001b[0m, in \u001b[0;36mSQVAE.val_model\u001b[1;34m(self, dataloader, device)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples_z, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    384\u001b[0m     samples_z \u001b[38;5;241m=\u001b[39m samples_z[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Ensure it is a Tensor\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m spk, mem, mean, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m l \u001b[38;5;241m=\u001b[39m spk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    388\u001b[0m prob \u001b[38;5;241m=\u001b[39m spk\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m l\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\OneDrive\\Studie\\Embedded_Systems\\Thesis\\Code\\Python\\src\\QSVAE_model.py:88\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 88\u001b[0m     spk, mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     mean, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_mean_var(spk)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# self.plot_spikes(spk, mem, cur)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\OneDrive\\Studie\\Embedded_Systems\\Thesis\\Code\\Python\\src\\QSVAE_model.py:132\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    128\u001b[0m   mem2_rec\u001b[38;5;241m.\u001b[39mappend(mem2)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m mem1, mem2, spk1, spk2, cur1, cur2\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspk2_rec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mstack(mem2_rec, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 3.17 GiB is allocated by PyTorch, and 982.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Reproduction of paper\n",
    "parameters = [\n",
    "    (3, 100, 20_000),\n",
    "    (4, 200, 100_000),\n",
    "    (5, 500, 4**5 * 500),  # 4^5 * 500\n",
    "    (6, 600, 4**6 * 500),  # 4^6 * 500\n",
    "    (7, 800, 4**7 * 500),  # 4^7 * 500\n",
    "    (8, 1000, 4**8 * 500)  # 4^8 * 500\n",
    "]\n",
    "\n",
    "# Run the model for each parameter setting and calculate fidelity:\n",
    "fidelities = []\n",
    "for param in parameters:\n",
    "    n, batch_train, batch_val = param  # Unpack parameters\n",
    "    quantum_exp = QuantumExperiment(backend, n, shots)\n",
    "    quantum_exp.run_experiment()\n",
    "#     quantum_exp.print_circuits_with_counts()\n",
    "    train_loader, test_loader, val_loader = load_data(quantum_exp,\n",
    "                                                      first_run,\n",
    "                                                      backend,\n",
    "                                                      n,\n",
    "                                                      shots,\n",
    "                                                      split,\n",
    "                                                      [batch_train, batch_test, batch_val],\n",
    "                                                      shuffle, num_workers)\n",
    "\n",
    "    # Instantiate the model for the given parameters\n",
    "    model = SQVAE(n=n, batch_size=[batch_train, batch_train, batch_val],\n",
    "                beta=beta, num_steps=num_steps, learning_rate=learning_rate, device=device)\n",
    "\n",
    "    # Run the model and get the fidelity for the current parameter setting\n",
    "    fidelity_score = model.run(train=True, test=False, val=True,\n",
    "                            train_loader=train_loader, test_loader=test_loader,\n",
    "                            val_loader=val_loader, num_epochs=num_epochs)\n",
    "    # Append the fidelity score to the list\n",
    "    fidelities.append(fidelity_score)\n",
    "\n",
    "# Plot the histogram of fidelities\n",
    "plot_histogram(fidelities, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "215vznl_mCQn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
